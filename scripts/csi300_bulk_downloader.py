#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ²ªæ·±300äº”å¹´æ•°æ®æ‰¹é‡ä¸‹è½½å™¨
ä¸“ä¸ºå¤§è§„æ¨¡å†å²æ•°æ®å›æµ‹ç ”ç©¶è®¾è®¡
"""

import sys
import time
import pandas as pd
import argparse
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
import logging
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(threadName)s] %(message)s',
    handlers=[
        logging.FileHandler('csi300_bulk_download.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "backend"))

# å¯¼å…¥æ•°æ®æºå®¢æˆ·ç«¯
try:
    from app.services.data_acquisition.tushare_client import TushareDataAcquirer
    TUSHARE_AVAILABLE = True
except ImportError:
    TUSHARE_AVAILABLE = False
    logger.error("Tushareå®¢æˆ·ç«¯ä¸å¯ç”¨")

try:
    import akshare as ak
    AKSHARE_AVAILABLE = True
except ImportError:
    AKSHARE_AVAILABLE = False
    logger.error("AkShareä¸å¯ç”¨")


class ProgressTracker:
    """ä¸‹è½½è¿›åº¦è·Ÿè¸ªå™¨"""

    def __init__(self):
        self.lock = threading.Lock()
        self.total_stocks = 0
        self.completed_stocks = 0
        self.failed_stocks = 0
        self.successful_downloads = {}
        self.failed_downloads = {}
        self.start_time = time.time()

    def set_total(self, total: int):
        """è®¾ç½®æ€»è‚¡ç¥¨æ•°é‡"""
        with self.lock:
            self.total_stocks = total

    def add_success(self, stock_code: str, record_count: int, source: str):
        """æ·»åŠ æˆåŠŸä¸‹è½½è®°å½•"""
        with self.lock:
            self.completed_stocks += 1
            self.successful_downloads[stock_code] = {
                'records': record_count,
                'source': source,
                'timestamp': datetime.now().isoformat()
            }

    def add_failure(self, stock_code: str, error: str):
        """æ·»åŠ å¤±è´¥ä¸‹è½½è®°å½•"""
        with self.lock:
            self.failed_stocks += 1
            self.failed_downloads[stock_code] = {
                'error': error,
                'timestamp': datetime.now().isoformat()
            }

    def get_progress(self) -> Dict:
        """è·å–å½“å‰è¿›åº¦"""
        with self.lock:
            elapsed = time.time() - self.start_time
            progress_rate = self.completed_stocks / self.total_stocks if self.total_stocks > 0 else 0
            estimated_total = elapsed / progress_rate if progress_rate > 0 else 0
            remaining = estimated_total - elapsed

            return {
                'total': self.total_stocks,
                'completed': self.completed_stocks,
                'failed': self.failed_stocks,
                'progress_rate': progress_rate,
                'elapsed_time': elapsed,
                'estimated_remaining': remaining,
                'success_rate': self.completed_stocks / (self.completed_stocks + self.failed_stocks) if (self.completed_stocks + self.failed_stocks) > 0 else 0
            }

    def save_progress(self, filename: str):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        with self.lock:
            progress_data = {
                'progress': self.get_progress(),
                'successful_downloads': self.successful_downloads,
                'failed_downloads': self.failed_downloads,
                'timestamp': datetime.now().isoformat()
            }

            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(progress_data, f, indent=2, ensure_ascii=False)


class CSI300BulkDownloader:
    """æ²ªæ·±300æ‰¹é‡æ•°æ®ä¸‹è½½å™¨"""

    def __init__(self, primary_source: str = "tushare", data_dir: str = None):
        """
        åˆå§‹åŒ–ä¸‹è½½å™¨

        Args:
            primary_source: é¦–é€‰æ•°æ®æº ("tushare" æˆ– "akshare")
            data_dir: æ•°æ®å­˜å‚¨ç›®å½•
        """
        self.primary_source = primary_source
        self.data_dir = Path(data_dir) if data_dir else Path("data/historical/stocks/csi300_5year")
        self.data_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºå­ç›®å½•ç»“æ„
        self.stocks_dir = self.data_dir / "stocks"
        self.meta_dir = self.data_dir / "meta"
        self.reports_dir = self.data_dir / "reports"

        for dir_path in [self.stocks_dir, self.meta_dir, self.reports_dir]:
            dir_path.mkdir(exist_ok=True)

        # åˆå§‹åŒ–è¿›åº¦è·Ÿè¸ªå™¨
        self.progress = ProgressTracker()

        # ä¸‹è½½å‚æ•°
        self.api_delay = 0.5  # APIè°ƒç”¨é—´éš”
        self.batch_delay = 60  # æ‰¹æ¬¡é—´éš”
        self.batch_size = 50   # æ¯æ‰¹æ¬¡è‚¡ç¥¨æ•°é‡
        self.max_workers = 3   # å¹¶å‘çº¿ç¨‹æ•°

        # åˆå§‹åŒ–æ•°æ®æº
        self.data_sources = []
        if TUSHARE_AVAILABLE:
            try:
                self.tushare_client = TushareDataAcquirer(str(self.data_dir))
                self.data_sources.append("tushare")
            except Exception as e:
                logger.warning(f"Tushareåˆå§‹åŒ–å¤±è´¥: {e}")

        if AKSHARE_AVAILABLE:
            self.data_sources.append("akshare")

        logger.info(f"å¯ç”¨æ•°æ®æº: {', '.join(self.data_sources)}")
        logger.info(f"é¦–é€‰æ•°æ®æº: {primary_source}")

    def get_csi300_stocks(self) -> List[str]:
        """è·å–æ²ªæ·±300æˆåˆ†è‚¡åˆ—è¡¨"""
        logger.info("è·å–æ²ªæ·±300æˆåˆ†è‚¡åˆ—è¡¨...")

        # ä¼˜å…ˆä½¿ç”¨Tushareè·å–æœ€æ–°çš„æˆåˆ†è‚¡
        if "tushare" in self.data_sources:
            try:
                stocks = self.tushare_client.get_csi300_stocks()
                if stocks:
                    logger.info(f"ä»Tushareè·å–åˆ° {len(stocks)} åªæ²ªæ·±300æˆåˆ†è‚¡")
                    # ä¿å­˜æˆåˆ†è‚¡åˆ—è¡¨
                    with open(self.meta_dir / "csi300_list.json", 'w', encoding='utf-8') as f:
                        json.dump({
                            'source': 'tushare',
                            'stocks': stocks,
                            'count': len(stocks),
                            'timestamp': datetime.now().isoformat()
                        }, f, indent=2, ensure_ascii=False)
                    return stocks
            except Exception as e:
                logger.error(f"ä»Tushareè·å–æˆåˆ†è‚¡å¤±è´¥: {e}")

        # å¤‡ç”¨ï¼šä½¿ç”¨é¢„å®šä¹‰çš„æ²ªæ·±300æ ·æœ¬
        backup_stocks = self._get_backup_csi300_sample()
        logger.warning(f"ä½¿ç”¨å¤‡ç”¨æ ·æœ¬è‚¡ç¥¨: {len(backup_stocks)} åª")
        return backup_stocks

    def _get_backup_csi300_sample(self) -> List[str]:
        """è·å–å¤‡ç”¨æ²ªæ·±300æ ·æœ¬"""
        # åŸºäºæ²ªæ·±300å„è¡Œä¸šæƒé‡çš„ä»£è¡¨æ€§æ ·æœ¬
        sample_stocks = [
            # é‡‘èåœ°äº§ (æƒé‡è¾ƒé«˜)
            '000001', '600000', '600036', '601318', '601398', '000002',
            '600048', '000069', '600016', '601166', '601328', '000651',

            # ä¸»è¦æ¶ˆè´¹
            '600519', '000858', '000568', '600779', '600887', '000895',
            '002304', '600600', '000596', '000799', '600779', '600597',

            # åŒ»è¯ç”Ÿç‰©
            '000423', '600276', '000661', '300750', '002007', '002422',
            '000538', '600196', '002038', '300122', '300015', '000999',

            # ç§‘æŠ€æˆé•¿
            '000063', '002415', '300750', '000725', '002230', '300017',
            '000977', '002410', '300144', '000100', '002024', '000988',

            # å‘¨æœŸè¡Œä¸š
            '600309', '002648', '000425', '002031', '600031', '000157',
            '002142', '600585', '000630', '601899', '600019', '000709',

            # ç¨³å®šå¢é•¿
            '000069', '600048', '000002', '600048', '000069', '600048',
            '000651', '002415', '000063', '600519', '000858', '600887'
        ]

        # å»é‡å¹¶è¿”å›
        return list(set(sample_stocks))

    def download_single_stock(self, stock_code: str, start_date: str, end_date: str) -> bool:
        """ä¸‹è½½å•åªè‚¡ç¥¨æ•°æ®"""
        logger.debug(f"å¼€å§‹ä¸‹è½½ {stock_code}")

        # æŒ‰ä¼˜å…ˆçº§å°è¯•æ•°æ®æº
        sources_order = [self.primary_source]
        for source in self.data_sources:
            if source != self.primary_source:
                sources_order.append(source)

        for source in sources_order:
            try:
                if source == "tushare":
                    df = self.tushare_client.get_stock_daily_data(stock_code, start_date, end_date)
                elif source == "akshare":
                    df = self._download_with_akshare(stock_code, start_date, end_date)
                else:
                    continue

                if not df.empty:
                    # ä¿å­˜æ•°æ®
                    success = self._save_stock_data(stock_code, df)
                    if success:
                        self.progress.add_success(stock_code, len(df), source)
                        logger.info(f"[OK] {stock_code} ä¸‹è½½æˆåŠŸ: {len(df)} æ¡è®°å½• (æ¥æº: {source})")
                        return True
                    else:
                        logger.error(f"[ERROR] {stock_code} ä¿å­˜å¤±è´¥")

            except Exception as e:
                logger.error(f"[ERROR] {stock_code} ä½¿ç”¨ {source} ä¸‹è½½å¤±è´¥: {e}")
                continue

        # æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥
        self.progress.add_failure(stock_code, "æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥")
        logger.error(f"[ERROR] {stock_code} æ‰€æœ‰æ•°æ®æºéƒ½å¤±è´¥")
        return False

    def _download_with_akshare(self, stock_code: str, start_date: str, end_date: str) -> pd.DataFrame:
        """ä½¿ç”¨AkShareä¸‹è½½æ•°æ®"""
        # AkShareéœ€è¦æ—¥æœŸæ ¼å¼è½¬æ¢ YYYY-MM-DD
        start_date_formatted = start_date.replace('-', '')
        end_date_formatted = end_date.replace('-', '')

        df = ak.stock_zh_a_hist(
            symbol=stock_code,
            period="daily",
            start_date=start_date_formatted,
            end_date=end_date_formatted,
            adjust="qfq"
        )

        if df.empty:
            return pd.DataFrame()

        # æ ‡å‡†åŒ–åˆ—å
        df = df.rename(columns={
            'æ—¥æœŸ': 'date',
            'å¼€ç›˜': 'open',
            'æ”¶ç›˜': 'close',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount'
        })

        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        df['date'] = pd.to_datetime(df['date'])
        for col in ['open', 'close', 'high', 'low']:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')
        df['amount'] = pd.to_numeric(df['amount'], errors='coerce')

        # æ·»åŠ è‚¡ç¥¨ä»£ç 
        df['stock_code'] = stock_code

        # æŒ‰æ—¥æœŸæ’åº
        df = df.sort_values('date').reset_index(drop=True)

        return df

    def _save_stock_data(self, stock_code: str, df: pd.DataFrame) -> bool:
        """ä¿å­˜è‚¡ç¥¨æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            if df.empty:
                return False

            # æŒ‰å¹´ä»½åˆ†ç›®å½•å­˜å‚¨
            df['year'] = df['date'].dt.year

            for year, year_data in df.groupby('year'):
                year_dir = self.stocks_dir / str(year)
                year_dir.mkdir(exist_ok=True)

                filename = year_dir / f"{stock_code}.csv"
                year_data = year_data.drop('year', axis=1)

                # å¦‚æœæ–‡ä»¶å­˜åœ¨ï¼Œåˆå¹¶å¹¶å»é‡
                if filename.exists():
                    existing_df = pd.read_csv(filename)
                    existing_df['date'] = pd.to_datetime(existing_df['date'])
                    combined_df = pd.concat([existing_df, year_data], ignore_index=True)
                    combined_df = combined_df.drop_duplicates(subset=['date'], keep='last')
                    combined_df = combined_df.sort_values('date')
                    combined_df.to_csv(filename, index=False)
                else:
                    year_data.to_csv(filename, index=False)

            return True

        except Exception as e:
            logger.error(f"ä¿å­˜ {stock_code} æ•°æ®å¤±è´¥: {e}")
            return False

    def download_csi300_bulk(self, start_date: str, end_date: str, max_stocks: int = None):
        """æ‰¹é‡ä¸‹è½½æ²ªæ·±300æ•°æ®"""
        logger.info("å¼€å§‹æ²ªæ·±300äº”å¹´æ•°æ®æ‰¹é‡ä¸‹è½½")
        logger.info(f"æ—¶é—´èŒƒå›´: {start_date} åˆ° {end_date}")
        logger.info(f"æ•°æ®ç›®å½•: {self.data_dir}")

        # è·å–æ²ªæ·±300æˆåˆ†è‚¡
        stock_list = self.get_csi300_stocks()

        if max_stocks:
            stock_list = stock_list[:max_stocks]

        self.progress.set_total(len(stock_list))
        logger.info(f"è®¡åˆ’ä¸‹è½½ {len(stock_list)} åªè‚¡ç¥¨æ•°æ®")

        # åˆ†æ‰¹ä¸‹è½½
        total_batches = (len(stock_list) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(stock_list))
            batch_stocks = stock_list[start_idx:end_idx]

            logger.info(f"å¼€å§‹ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹æ¬¡: {len(batch_stocks)} åªè‚¡ç¥¨")

            # å¹¶å‘ä¸‹è½½å½“å‰æ‰¹æ¬¡
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = {
                    executor.submit(self.download_single_stock, stock_code, start_date, end_date): stock_code
                    for stock_code in batch_stocks
                }

                for future in as_completed(futures):
                    stock_code = futures[future]
                    try:
                        future.result()
                        time.sleep(self.api_delay)  # APIé™æµ
                    except Exception as e:
                        logger.error(f"å¤„ç† {stock_code} æ—¶å‡ºç°å¼‚å¸¸: {e}")

            # æ‰¹æ¬¡é—´ä¼‘æ¯
            if batch_idx < total_batches - 1:
                logger.info(f"æ‰¹æ¬¡ {batch_idx + 1} å®Œæˆï¼Œä¼‘æ¯ {self.batch_delay} ç§’...")
                time.sleep(self.batch_delay)

            # ä¿å­˜è¿›åº¦
            self.progress.save_progress(self.meta_dir / f"progress_batch_{batch_idx + 1}.json")

            # æ‰“å°å½“å‰è¿›åº¦
            progress_info = self.progress.get_progress()
            logger.info(f"å½“å‰è¿›åº¦: {progress_info['completed']}/{progress_info['total']} "
                       f"({progress_info['progress_rate']:.1%}) "
                       f"æˆåŠŸç‡: {progress_info['success_rate']:.1%}")

        # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report()

    def _generate_final_report(self):
        """ç”Ÿæˆæœ€ç»ˆä¸‹è½½æŠ¥å‘Š"""
        progress_info = self.progress.get_progress()

        report = {
            'download_summary': {
                'total_stocks': progress_info['total'],
                'successful_downloads': progress_info['completed'],
                'failed_downloads': progress_info['failed'],
                'success_rate': progress_info['success_rate'],
                'total_time_seconds': progress_info['elapsed_time']
            },
            'successful_stocks': self.progress.successful_downloads,
            'failed_stocks': self.progress.failed_downloads,
            'data_sources': self.data_sources,
            'primary_source': self.primary_source,
            'download_config': {
                'api_delay': self.api_delay,
                'batch_size': self.batch_size,
                'max_workers': self.max_workers
            },
            'timestamp': datetime.now().isoformat()
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"csi300_download_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        logger.info(f"[COMPLETE] ä¸‹è½½å®Œæˆ! æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info(f"[SUMMARY] æ€»è®¡: {progress_info['total']} åªè‚¡ç¥¨")
        logger.info(f"[SUCCESS] æˆåŠŸ: {progress_info['completed']} åª")
        logger.info(f"[FAILED] å¤±è´¥: {progress_info['failed']} åª")
        logger.info(f"[RATE] æˆåŠŸç‡: {progress_info['success_rate']:.1%}")
        logger.info(f"[TIME] æ€»è€—æ—¶: {progress_info['elapsed_time']:.1f} ç§’")


def main():
    parser = argparse.ArgumentParser(description="æ²ªæ·±300äº”å¹´æ•°æ®æ‰¹é‡ä¸‹è½½å™¨")
    parser.add_argument("--source", choices=["tushare", "akshare"], default="tushare",
                       help="é¦–é€‰æ•°æ®æº")
    parser.add_argument("--start-date", type=str, default="2019-01-01",
                       help="å¼€å§‹æ—¥æœŸ YYYY-MM-DD")
    parser.add_argument("--end-date", type=str,
                       default=datetime.now().strftime('%Y-%m-%d'),
                       help="ç»“æŸæ—¥æœŸ YYYY-MM-DD")
    parser.add_argument("--max-stocks", type=int, help="æœ€å¤§ä¸‹è½½æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰")
    parser.add_argument("--data-dir", type=str, help="æ•°æ®å­˜å‚¨ç›®å½•")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼ï¼ˆä¸‹è½½5åªè‚¡ç¥¨ï¼‰")

    args = parser.parse_args()

    print("æ²ªæ·±300äº”å¹´æ•°æ®æ‰¹é‡ä¸‹è½½å™¨")
    print("=" * 60)

    try:
        # åˆ›å»ºä¸‹è½½å™¨
        downloader = CSI300BulkDownloader(
            primary_source=args.source,
            data_dir=args.data_dir
        )

        # æµ‹è¯•æ¨¡å¼
        if args.test:
            max_stocks = 5
            end_date = datetime.now().strftime('%Y-%m-%d')
            start_date = (datetime.now() - timedelta(days=365)).strftime('%Y-%m-%d')
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼: ä¸‹è½½5åªè‚¡ç¥¨1å¹´æ•°æ®")
        else:
            max_stocks = args.max_stocks
            start_date = args.start_date
            end_date = args.end_date

        # å¼€å§‹æ‰¹é‡ä¸‹è½½
        downloader.download_csi300_bulk(start_date, end_date, max_stocks)

    except Exception as e:
        logger.error(f"ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()