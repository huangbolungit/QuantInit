#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡éªŒè¯è„šæœ¬
éªŒè¯æ²ªæ·±300è‚¡ç¥¨æ•°æ®çš„å®Œæ•´æ€§å’Œè´¨é‡
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any
import logging
import json
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class DataQualityValidator:
    """æ•°æ®è´¨é‡éªŒè¯å™¨"""

    def __init__(self, data_dir: str = None):
        self.data_dir = Path(data_dir) if data_dir else Path("data/historical/stocks/csi300_5year")
        self.stocks_dir = self.data_dir / "stocks"
        self.reports_dir = self.data_dir / "reports"
        self.plots_dir = self.data_dir / "plots"

        # åˆ›å»ºæŠ¥å‘Šå’Œå›¾è¡¨ç›®å½•
        for dir_path in [self.reports_dir, self.plots_dir]:
            dir_path.mkdir(exist_ok=True)

        logger.info(f"æ•°æ®è´¨é‡éªŒè¯å™¨åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")

    def load_stock_data(self, stock_code: str) -> pd.DataFrame:
        """åŠ è½½å•åªè‚¡ç¥¨çš„æ‰€æœ‰å¹´ä»½æ•°æ®"""
        all_data = []

        # éå†æ‰€æœ‰å¹´ä»½ç›®å½•
        for year_dir in sorted(self.stocks_dir.iterdir()):
            if year_dir.is_dir() and year_dir.name.isdigit():
                file_path = year_dir / f"{stock_code}.csv"
                if file_path.exists():
                    try:
                        df = pd.read_csv(file_path)
                        df['date'] = pd.to_datetime(df['date'])
                        all_data.append(df)
                    except Exception as e:
                        logger.warning(f"è¯»å– {file_path} å¤±è´¥: {e}")

        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            combined_df = combined_df.sort_values('date').drop_duplicates(subset=['date'], keep='last')
            return combined_df
        else:
            return pd.DataFrame()

    def validate_single_stock(self, stock_code: str) -> Dict[str, Any]:
        """éªŒè¯å•åªè‚¡ç¥¨çš„æ•°æ®è´¨é‡"""
        try:
            df = self.load_stock_data(stock_code)

            if df.empty:
                return {
                    'stock_code': stock_code,
                    'status': 'no_data',
                    'total_records': 0,
                    'date_range': None,
                    'missing_values': {},
                    'quality_score': 0.0
                }

            # åŸºæœ¬ç»Ÿè®¡
            total_records = len(df)
            date_range = {
                'start': df['date'].min().strftime('%Y-%m-%d'),
                'end': df['date'].max().strftime('%Y-%m-%d'),
                'trading_days': total_records
            }

            # ç¼ºå¤±å€¼æ£€æŸ¥
            missing_values = df.isnull().sum().to_dict()

            # ä»·æ ¼æ•°æ®åˆç†æ€§æ£€æŸ¥
            price_issues = 0
            for col in ['open', 'high', 'low', 'close']:
                if col in df.columns:
                    price_issues += (df[col] <= 0).sum()
                    price_issues += (df[col] > 10000).sum()  # å¼‚å¸¸é«˜ä»·

            # æˆäº¤é‡æ£€æŸ¥
            volume_issues = 0
            if 'volume' in df.columns:
                volume_issues += (df['volume'] < 0).sum()

            # æ•°æ®è¿ç»­æ€§æ£€æŸ¥
            df_sorted = df.sort_values('date')
            df_sorted['date_diff'] = df_sorted['date'].diff().dt.days
            large_gaps = (df_sorted['date_diff'] > 15).sum()  # è¶…è¿‡15å¤©çš„é—´éš”

            # è®¡ç®—è´¨é‡åˆ†æ•°
            quality_score = 1.0
            quality_score -= (price_issues / total_records) * 0.3
            quality_score -= (large_gaps / total_records) * 0.2
            quality_score -= (df.isnull().sum().sum() / (total_records * len(df.columns))) * 0.3
            quality_score -= (volume_issues / total_records) * 0.2

            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            required_columns = ['date', 'open', 'high', 'low', 'close', 'volume']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                quality_score -= len(missing_columns) * 0.1

            quality_score = max(0, min(1, quality_score))

            # è®¡ç®—æ”¶ç›Šç‡ç»Ÿè®¡
            if 'close' in df.columns and len(df) > 1:
                df['daily_return'] = df['close'].pct_change()
                return_stats = {
                    'mean_daily_return': df['daily_return'].mean(),
                    'std_daily_return': df['daily_return'].std(),
                    'max_return': df['daily_return'].max(),
                    'min_return': df['daily_return'].min(),
                    'skewness': df['daily_return'].skew(),
                    'kurtosis': df['daily_return'].kurtosis()
                }
            else:
                return_stats = {}

            return {
                'stock_code': stock_code,
                'status': 'valid',
                'total_records': total_records,
                'date_range': date_range,
                'missing_values': {k: v for k, v in missing_values.items() if v > 0},
                'price_issues': price_issues,
                'volume_issues': volume_issues,
                'data_gaps': large_gaps,
                'missing_columns': missing_columns,
                'quality_score': quality_score,
                'return_statistics': return_stats
            }

        except Exception as e:
            logger.error(f"éªŒè¯ {stock_code} æ—¶å‡ºé”™: {e}")
            return {
                'stock_code': stock_code,
                'status': 'error',
                'error': str(e),
                'quality_score': 0.0
            }

    def validate_all_stocks(self) -> Dict[str, Any]:
        """éªŒè¯æ‰€æœ‰è‚¡ç¥¨æ•°æ®è´¨é‡"""
        logger.info("å¼€å§‹éªŒè¯æ‰€æœ‰è‚¡ç¥¨æ•°æ®è´¨é‡...")

        # è·å–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
        stock_files = list(self.stocks_dir.rglob("*.csv"))
        stock_codes = set()
        for file_path in stock_files:
            stock_code = file_path.stem
            if stock_code.isdigit() and len(stock_code) == 6:
                stock_codes.add(stock_code)

        stock_codes = sorted(list(stock_codes))
        logger.info(f"å‘ç° {len(stock_codes)} åªè‚¡ç¥¨çš„æ•°æ®")

        validation_results = {}
        quality_scores = []
        total_records = []

        for i, stock_code in enumerate(stock_codes, 1):
            if i % 10 == 0:
                logger.info(f"éªŒè¯è¿›åº¦: {i}/{len(stock_codes)}")

            result = self.validate_single_stock(stock_code)
            validation_results[stock_code] = result

            if result['status'] == 'valid':
                quality_scores.append(result['quality_score'])
                total_records.append(result['total_records'])

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        if quality_scores:
            summary_stats = {
                'total_stocks': len(stock_codes),
                'valid_stocks': len([r for r in validation_results.values() if r['status'] == 'valid']),
                'invalid_stocks': len([r for r in validation_results.values() if r['status'] != 'valid']),
                'avg_quality_score': np.mean(quality_scores),
                'min_quality_score': np.min(quality_scores),
                'max_quality_score': np.max(quality_scores),
                'avg_records_per_stock': np.mean(total_records),
                'total_records_all_stocks': np.sum(total_records),
                'quality_distribution': {
                    'excellent (>0.9)': len([s for s in quality_scores if s > 0.9]),
                    'good (0.7-0.9)': len([s for s in quality_scores if 0.7 <= s <= 0.9]),
                    'fair (0.5-0.7)': len([s for s in quality_scores if 0.5 <= s < 0.7]),
                    'poor (<0.5)': len([s for s in quality_scores if s < 0.5])
                }
            }
        else:
            summary_stats = {
                'total_stocks': len(stock_codes),
                'valid_stocks': 0,
                'invalid_stocks': len(stock_codes),
                'avg_quality_score': 0,
                'message': 'No valid data found'
            }

        report = {
            'summary': summary_stats,
            'individual_stocks': validation_results,
            'validation_timestamp': datetime.now().isoformat()
        }

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"data_quality_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"æ•°æ®è´¨é‡éªŒè¯å®Œæˆï¼ŒæŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        logger.info(f"æ€»è®¡: {summary_stats['total_stocks']} åªè‚¡ç¥¨")
        logger.info(f"æœ‰æ•ˆ: {summary_stats['valid_stocks']} åª")
        logger.info(f"å¹³å‡è´¨é‡åˆ†æ•°: {summary_stats['avg_quality_score']:.3f}")

        return report

    def create_quality_plots(self, report: Dict[str, Any]):
        """åˆ›å»ºæ•°æ®è´¨é‡å¯è§†åŒ–å›¾è¡¨"""
        logger.info("ç”Ÿæˆæ•°æ®è´¨é‡å¯è§†åŒ–å›¾è¡¨...")

        individual_stocks = report['individual_stocks']
        valid_stocks = {k: v for k, v in individual_stocks.items() if v['status'] == 'valid'}

        if not valid_stocks:
            logger.warning("æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œæ— æ³•ç”Ÿæˆå›¾è¡¨")
            return

        # æå–æ•°æ®
        stock_codes = list(valid_stocks.keys())
        quality_scores = [valid_stocks[code]['quality_score'] for code in stock_codes]
        total_records = [valid_stocks[code]['total_records'] for code in stock_codes]

        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ²ªæ·±300è‚¡ç¥¨æ•°æ®è´¨é‡åˆ†æ', fontsize=16, fontweight='bold')

        # 1. è´¨é‡åˆ†æ•°åˆ†å¸ƒç›´æ–¹å›¾
        axes[0, 0].hist(quality_scores, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        axes[0, 0].set_title('è´¨é‡åˆ†æ•°åˆ†å¸ƒ')
        axes[0, 0].set_xlabel('è´¨é‡åˆ†æ•°')
        axes[0, 0].set_ylabel('è‚¡ç¥¨æ•°é‡')
        axes[0, 0].axvline(np.mean(quality_scores), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(quality_scores):.3f}')
        axes[0, 0].legend()

        # 2. è®°å½•æ•°é‡åˆ†å¸ƒ
        axes[0, 1].hist(total_records, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        axes[0, 1].set_title('äº¤æ˜“è®°å½•æ•°é‡åˆ†å¸ƒ')
        axes[0, 1].set_xlabel('è®°å½•æ•°é‡')
        axes[0, 1].set_ylabel('è‚¡ç¥¨æ•°é‡')
        axes[0, 1].axvline(np.mean(total_records), color='red', linestyle='--', label=f'å¹³å‡å€¼: {np.mean(total_records):.0f}')
        axes[0, 1].legend()

        # 3. è´¨é‡åˆ†æ•° vs è®°å½•æ•°é‡æ•£ç‚¹å›¾
        axes[1, 0].scatter(total_records, quality_scores, alpha=0.6, color='purple')
        axes[1, 0].set_title('è´¨é‡åˆ†æ•° vs è®°å½•æ•°é‡')
        axes[1, 0].set_xlabel('è®°å½•æ•°é‡')
        axes[1, 0].set_ylabel('è´¨é‡åˆ†æ•°')

        # æ·»åŠ è¶‹åŠ¿çº¿
        z = np.polyfit(total_records, quality_scores, 1)
        p = np.poly1d(z)
        axes[1, 0].plot(total_records, p(total_records), "r--", alpha=0.8)

        # 4. è´¨é‡ç­‰çº§é¥¼å›¾
        quality_dist = report['summary']['quality_distribution']
        labels = list(quality_dist.keys())
        sizes = list(quality_dist.values())
        colors = ['#ff9999', '#66b3ff', '#99ff99', '#ffcc99']

        axes[1, 1].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
        axes[1, 1].set_title('æ•°æ®è´¨é‡ç­‰çº§åˆ†å¸ƒ')

        plt.tight_layout()
        plot_file = self.plots_dir / f"data_quality_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()

        logger.info(f"è´¨é‡åˆ†æå›¾è¡¨å·²ä¿å­˜: {plot_file}")


def main():
    print("æ²ªæ·±300è‚¡ç¥¨æ•°æ®è´¨é‡éªŒè¯")
    print("=" * 50)

    try:
        # åˆ›å»ºéªŒè¯å™¨
        validator = DataQualityValidator()

        # è¿è¡ŒéªŒè¯
        report = validator.validate_all_stocks()

        # ç”Ÿæˆå›¾è¡¨
        validator.create_quality_plots(report)

        # æ‰“å°ç®€è¦ç»“æœ
        summary = report['summary']
        print(f"\nğŸ“Š æ•°æ®è´¨é‡éªŒè¯ç»“æœ:")
        print(f"æ€»è‚¡ç¥¨æ•°: {summary['total_stocks']}")
        print(f"æœ‰æ•ˆè‚¡ç¥¨: {summary['valid_stocks']}")
        print(f"æ— æ•ˆè‚¡ç¥¨: {summary['invalid_stocks']}")
        print(f"å¹³å‡è´¨é‡åˆ†æ•°: {summary['avg_quality_score']:.3f}")
        print(f"å¹³å‡æ¯åªè‚¡ç¥¨è®°å½•æ•°: {summary['avg_records_per_stock']:.0f}")

        if 'quality_distribution' in summary:
            print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
            for level, count in summary['quality_distribution'].items():
                print(f"  {level}: {count} åª")

    except Exception as e:
        logger.error(f"æ•°æ®è´¨é‡éªŒè¯å¤±è´¥: {e}")
        return 1

    return 0


if __name__ == "__main__":
    exit(main())