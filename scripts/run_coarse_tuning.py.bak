#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Coarse Tuning Runner

ç›®æ ‡ï¼šå¯¹æ ¸å¿ƒå‡å€¼å›å½’ç­–ç•¥å‚æ•°è¿›è¡Œç¬¬ä¸€é˜¶æ®µâ€œç²—è°ƒâ€ï¼Œé‡‡ç”¨æ— å‰è§†åå·®å›æµ‹å¼•æ“ä¸ä¸¥æ ¼æ•°æ®åˆ‡åˆ†ã€?
ç‰¹æ€§ï¼š
- è‡ªåŠ¨å‡†å¤‡æ•°æ®ï¼ˆAkShare ä¼˜å…ˆï¼‰ï¼ŒæŒ‰å¹´è½åˆ° data/historical/stocks/complete_csi800/stocks/{YYYY}/{code}.csv
- è®­ç»ƒé›†ä¸éªŒè¯é›†åˆ†ç¦»ï¼ˆé»˜è®¤ï¼štrain=2022-01-01..2023-12-31ï¼Œvalid=2024-01-01..è‡³ä»Šï¼?- ç½‘æ ¼ï¼šlookback_period Ã— buy_threshold Ã— sell_thresholdï¼ˆå¯æŒ‰éœ€æ‰©å±•ï¼?- è¾“å‡ºï¼šJSON ä¸?Markdown æ‘˜è¦åˆ?optimization_results/coarse_tuning_{ts}/

ç”¨æ³•ï¼?  python scripts/run_coarse_tuning.py --pool 000001,000002,600036,600519,000858 \
    --train 2022-01-01,2023-12-31 --valid 2024-01-01,2024-12-31
"""

from __future__ import annotations

import argparse
import json
from datetime import datetime, date
from pathlib import Path
from typing import List, Dict, Any, Tuple

import pandas as pd
import sys
import os

try:
    import akshare as ak
    AK = True
except Exception:
    AK = False


DATA_ROOT = Path("data/historical/stocks/complete_csi800/stocks")
OUT_ROOT = Path("optimization_results")
OUT_ROOT.mkdir(exist_ok=True)


def _year_dir_for(dt: pd.Timestamp) -> Path:
    return DATA_ROOT / str(dt.year)


def fetch_and_store(code: str, start: str, end: str) -> int:
    if not AK:
        return 0
    df = ak.stock_zh_a_hist(symbol=code, period="daily", start_date=start.replace("-", ""), end_date=end.replace("-", ""), adjust="qfq")
    if df is None or df.empty:
        return 0
    # standardize
    df = df.rename(columns={
        'æ—¥æœŸ': 'date', 'å¼€ç›?: 'open', 'æ”¶ç›˜': 'close', 'æœ€é«?: 'high', 'æœ€ä½?: 'low', 'æˆäº¤é‡?: 'volume', 'æˆäº¤é¢?: 'amount'
    })
    df['date'] = pd.to_datetime(df['date'])
    df['stock_code'] = code
    # save split by year
    total = 0
    for y, g in df.groupby(df['date'].dt.year):
        ydir = DATA_ROOT / str(y)
        ydir.mkdir(parents=True, exist_ok=True)
        fp = ydir / f"{code}.csv"
        if fp.exists():
            old = pd.read_csv(fp)
            old['date'] = pd.to_datetime(old['date'])
            merged = pd.concat([old, g], ignore_index=True)
            merged = merged.drop_duplicates(subset=['date']).sort_values('date')
            merged.to_csv(fp, index=False)
            total += len(g)
        else:
            g.to_csv(fp, index=False)
            total += len(g)
    return total


def ensure_data(pool: List[str], start: str, end: str) -> None:
    for code in pool:
        try:
            cnt = fetch_and_store(code, start, end)
            print(f"[data] {code}: +{cnt} rows")
        except Exception as e:
            print(f"[data] {code}: failed {e}")


def run_grid(pool: List[str], start: str, end: str, grid: Dict[str, List[Any]], fixed: Dict[str, Any]) -> Dict[str, Any]:
    # ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨è·¯å¾„ä¸­ï¼Œæ”¯æŒä½œä¸ºå‘½ä»¤è¿è¡Œ
    sys.path.append(str(Path(__file__).parent.parent))
    try:
        from scripts.parameter_optimizer import ParameterOptimizationRunner as POR  # type: ignore
    except Exception:
        from parameter_optimizer import ParameterOptimizationRunner as POR  # type: ignore

    runner = POR()
    # compose config like file mode
    cfg = {
        "strategy_name": "OptimizedMeanReversion",
        "parameter_grid": grid,
        "fixed_parameters": fixed,
    }
    # Build a light args namespace
    class Args:
        def __init__(self):
            self.data_dir = str(DATA_ROOT)
            self.output_dir = str(OUT_ROOT)
            self.start_date = start
            self.end_date = end
            self.rebalancing_freq = 10
            self.stock_pool = ",".join(pool)
            self.verbose = False
            self.quiet = False

    args = Args()
    # The upstream runner exposes run_optimization(config, args)
    # It returns a summary dict
    result = runner.run_optimization(cfg, args)
    return result


def write_report(outdir: Path, tag: str, result: Dict[str, Any]) -> None:
    outdir.mkdir(parents=True, exist_ok=True)
    # JSON
    (outdir / f"{tag}_results.json").write_text(json.dumps(result, ensure_ascii=False, indent=2), encoding="utf-8")
    # Markdown brief
    lines = [f"# {tag} ç»“æœæ‘˜è¦",
             "", f"æ—¶é—´: {datetime.now().isoformat()}", "",
             f"æ€»ç»„å? {result.get('total_combinations', result.get('summary',{}).get('total_combinations'))}",
             f"æˆåŠŸæµ‹è¯•: {result.get('successful_tests', result.get('summary',{}).get('successful_tests'))}",
             ""]
    best = result.get('best_result') or {}
    if best:
        lines += ["## æœ€ä¼˜å‚æ•?, "", "```json", json.dumps(best.get('parameters', {}), ensure_ascii=False, indent=2), "```", "",
                  "## æŒ‡æ ‡", "", "```json", json.dumps(best.get('metrics', {}), ensure_ascii=False, indent=2), "```"]
    (outdir / f"{tag}_report.md").write_text("\n".join(lines), encoding="utf-8")


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="Coarse tuning runner (mean reversion)")
    p.add_argument("--pool", type=str, default="000001,000002,600036,600519,000858", help="è‚¡ç¥¨æ± ï¼Œé€—å·åˆ†éš”")
    p.add_argument("--train", type=str, default="2022-01-01,2023-12-31", help="è®­ç»ƒé›†åŒºé—´ï¼Œèµ·æ­¢ä»¥é€—å·åˆ†éš”")
    p.add_argument("--valid", type=str, default=f"2024-01-01,{date.today().isoformat()}", help="éªŒè¯é›†åŒºé—´ï¼Œèµ·æ­¢ä»¥é€—å·åˆ†éš”")
    return p.parse_args()


def main() -> None:
    args = parse_args()
    pool = [c.strip() for c in args.pool.split(',') if c.strip()]
    train_start, train_end = [s.strip() for s in args.train.split(',')]
    valid_start, valid_end = [s.strip() for s in args.valid.split(',')]

    print("[coarse] å‡†å¤‡æ•°æ®: train+valid è¦†ç›–â€?)
    ensure_data(pool, start=min(train_start, valid_start), end=max(train_end, valid_end))

    print("[coarse] è¿è¡Œè®­ç»ƒé›†ç²—è°ƒç½‘æ ¼â€?)
    grid = {
        "lookback_period": [5, 10, 15, 20, 30],
        "buy_threshold": [-0.03, -0.05, -0.08, -0.10],
        "sell_threshold": [0.02, 0.03, 0.05],
    }
    fixed = {"max_hold_days": 15}

    train_res = run_grid(pool, train_start, train_end, grid, fixed)

    print("[coarse] ä»¥è®­ç»ƒé›†æœ€ä¼˜å‚æ•°åœ¨éªŒè¯é›†è¯„ä¼°â€?)
    best = (train_res.get('best_result') or {}).get('parameters', {})
    if not best:
        print("[warn] è®­ç»ƒé›†æœªå¾—åˆ°æœ€ä¼˜å‚æ•°ï¼Œè·³è¿‡éªŒè¯è¯„ä¼°")
        best = {"lookback_period": 20, "buy_threshold": -0.08, "sell_threshold": 0.02}
    valid_grid = {k: [v] for k, v in best.items()}
    valid_res = run_grid(pool, valid_start, valid_end, valid_grid, fixed)

    ts = datetime.now().strftime('%Y%m%d_%H%M%S')
    outdir = OUT_ROOT / f"coarse_tuning_{ts}"
    write_report(outdir, "train", train_res)
    write_report(outdir, "valid", valid_res)
    print(f"[coarse] å®Œæˆã€‚æŠ¥å‘Šç›®å½? {outdir}")


if __name__ == "__main__":
    main()

