#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ²ªæ·±300äº”å¹´æ•°æ®å›æµ‹ç ”ç©¶æ¡†æ¶
åŸºäºçœŸå®å†å²æ•°æ®çš„é‡åŒ–ç­–ç•¥ç ”ç©¶å·¥å…·
"""

import sys
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
import logging
import json
import matplotlib.pyplot as plt
import seaborn as sns
from concurrent.futures import ProcessPoolExecutor
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('csi300_research.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent / "backend"))

from app.services.backtesting.engine import BacktestEngine
from app.services.factors.momentum import MomentumFactor
from app.services.factors.value import ValueFactor


class CSI300ResearchFramework:
    """æ²ªæ·±300å›æµ‹ç ”ç©¶æ¡†æ¶"""

    def __init__(self, data_dir: str = None):
        """
        åˆå§‹åŒ–ç ”ç©¶æ¡†æ¶

        Args:
            data_dir: æ•°æ®ç›®å½•è·¯å¾„
        """
        self.data_dir = Path(data_dir) if data_dir else Path("data/historical/stocks/csi300_5year")
        self.stocks_dir = self.data_dir / "stocks"
        self.reports_dir = self.data_dir / "reports"
        self.analysis_dir = self.data_dir / "analysis"
        self.plots_dir = self.data_dir / "plots"

        # åˆ›å»ºåˆ†æç›®å½•
        for dir_path in [self.analysis_dir, self.plots_dir]:
            dir_path.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å›æµ‹å¼•æ“
        self.backtest_engine = BacktestEngine(str(self.stocks_dir))

        # ç ”ç©¶é…ç½®
        self.research_config = {
            'base_period': ('2019-01-01', '2024-12-31'),  # 5å¹´åŸºç¡€æœŸ
            'test_period': ('2023-01-01', '2024-12-31'),   # 2å¹´æµ‹è¯•æœŸ
            'universe_size': 50,  # è‚¡ç¥¨æ± å¤§å°
            'rebalance_frequency': 'monthly',  # æœˆåº¦è°ƒä»“
            'factor_combinations': [
                {'momentum': 1.0, 'value': 0.0},      # çº¯åŠ¨é‡
                {'momentum': 0.0, 'value': 1.0},      # çº¯ä»·å€¼
                {'momentum': 0.7, 'value': 0.3},      # ååŠ¨é‡
                {'momentum': 0.3, 'value': 0.7},      # åä»·å€¼
                {'momentum': 0.5, 'value': 0.5},      # å¹³è¡¡
            ]
        }

        logger.info(f"ç ”ç©¶æ¡†æ¶åˆå§‹åŒ–å®Œæˆï¼Œæ•°æ®ç›®å½•: {self.data_dir}")

    def load_available_stocks(self) -> List[str]:
        """åŠ è½½å¯ç”¨çš„è‚¡ç¥¨åˆ—è¡¨"""
        logger.info("åŠ è½½å¯ç”¨è‚¡ç¥¨åˆ—è¡¨...")

        stock_files = list(self.stocks_dir.rglob("*.csv"))
        stock_codes = set()

        for file_path in stock_files:
            stock_code = file_path.stem
            # ç¡®ä¿æ˜¯6ä½æ•°å­—è‚¡ç¥¨ä»£ç 
            if stock_code.isdigit() and len(stock_code) == 6:
                stock_codes.add(stock_code)

        available_stocks = sorted(list(stock_codes))
        logger.info(f"æ‰¾åˆ° {len(available_stocks)} åªå¯ç”¨è‚¡ç¥¨")

        # ä¿å­˜è‚¡ç¥¨åˆ—è¡¨
        with open(self.analysis_dir / "available_stocks.json", 'w', encoding='utf-8') as f:
            json.dump({
                'count': len(available_stocks),
                'stocks': available_stocks,
                'timestamp': datetime.now().isoformat()
            }, f, indent=2, ensure_ascii=False)

        return available_stocks

    def validate_data_quality(self, stock_codes: List[str]) -> Dict[str, Any]:
        """éªŒè¯æ•°æ®è´¨é‡"""
        logger.info("å¼€å§‹æ•°æ®è´¨é‡éªŒè¯...")

        quality_report = {
            'total_stocks': len(stock_codes),
            'valid_stocks': [],
            'invalid_stocks': [],
            'data_coverage': {},
            'quality_metrics': {}
        }

        for stock_code in stock_codes:
            try:
                # åŠ è½½è¯¥è‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®
                stock_files = list(self.stocks_dir.rglob(f"{stock_code}.csv"))
                if not stock_files:
                    quality_report['invalid_stocks'].append({
                        'stock': stock_code,
                        'reason': 'æ— æ•°æ®æ–‡ä»¶'
                    })
                    continue

                # åˆå¹¶æ‰€æœ‰å¹´ä»½çš„æ•°æ®
                all_data = []
                for file_path in stock_files:
                    df = pd.read_csv(file_path)
                    df['date'] = pd.to_datetime(df['date'])
                    all_data.append(df)

                if not all_data:
                    quality_report['invalid_stocks'].append({
                        'stock': stock_code,
                        'reason': 'æ•°æ®æ–‡ä»¶ä¸ºç©º'
                    })
                    continue

                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df = combined_df.sort_values('date').drop_duplicates(subset=['date'])

                # æ•°æ®è´¨é‡æ£€æŸ¥
                total_records = len(combined_df)
                missing_values = combined_df.isnull().sum().to_dict()
                date_range = {
                    'start': combined_df['date'].min().strftime('%Y-%m-%d'),
                    'end': combined_df['date'].max().strftime('%Y-%m-%d'),
                    'trading_days': total_records
                }

                # æ£€æŸ¥ä»·æ ¼æ•°æ®åˆç†æ€§
                price_issues = 0
                for col in ['open', 'high', 'low', 'close']:
                    if col in combined_df.columns:
                        price_issues += (combined_df[col] <= 0).sum()
                        price_issues += (combined_df[col] > 10000).sum()

                # æ£€æŸ¥æ•°æ®è¿ç»­æ€§
                combined_df['date_diff'] = combined_df['date'].diff().dt.days
                large_gaps = (combined_df['date_diff'] > 10).sum()

                quality_score = 1.0
                quality_score -= (price_issues / total_records) * 0.3
                quality_score -= (large_gaps / total_records) * 0.2
                quality_score -= (combined_df.isnull().sum().sum() / (total_records * len(combined_df.columns))) * 0.5

                if quality_score > 0.7 and total_records > 200:  # è‡³å°‘200ä¸ªäº¤æ˜“æ—¥
                    quality_report['valid_stocks'].append({
                        'stock': stock_code,
                        'quality_score': quality_score,
                        'records': total_records,
                        'date_range': date_range,
                        'missing_data': missing_values,
                        'price_issues': price_issues,
                        'data_gaps': large_gaps
                    })
                else:
                    quality_report['invalid_stocks'].append({
                        'stock': stock_code,
                        'reason': f'è´¨é‡åˆ†æ•°è¿‡ä½: {quality_score:.2f}',
                        'records': total_records,
                        'quality_score': quality_score
                    })

            except Exception as e:
                quality_report['invalid_stocks'].append({
                    'stock': stock_code,
                    'reason': f'å¤„ç†å¼‚å¸¸: {str(e)}'
                })

        # ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡
        quality_report['data_coverage'] = {
            'valid_count': len(quality_report['valid_stocks']),
            'invalid_count': len(quality_report['invalid_stocks']),
            'validity_rate': len(quality_report['valid_stocks']) / len(stock_codes)
        }

        # è®¡ç®—å¹³å‡è´¨é‡æŒ‡æ ‡
        if quality_report['valid_stocks']:
            avg_records = np.mean([s['records'] for s in quality_report['valid_stocks']])
            avg_quality = np.mean([s['quality_score'] for s in quality_report['valid_stocks']])
            quality_report['quality_metrics'] = {
                'avg_trading_days': avg_records,
                'avg_quality_score': avg_quality
            }

        logger.info(f"æ•°æ®è´¨é‡éªŒè¯å®Œæˆ: {quality_report['data_coverage']['valid_count']}/{quality_report['total_stocks']} "
                   f"({quality_report['data_coverage']['validity_rate']:.1%}) è‚¡ç¥¨æ•°æ®è´¨é‡è‰¯å¥½")

        # ä¿å­˜è´¨é‡æŠ¥å‘Š
        with open(self.analysis_dir / "data_quality_report.json", 'w', encoding='utf-8') as f:
            json.dump(quality_report, f, indent=2, ensure_ascii=False)

        return quality_report

    def run_factor_analysis(self, stock_codes: List[str], start_date: str, end_date: str) -> Dict[str, Any]:
        """è¿è¡Œå› å­åˆ†æ"""
        logger.info(f"å¼€å§‹å› å­åˆ†æ: {start_date} åˆ° {end_date}")

        factor_results = {}

        # åˆå§‹åŒ–å› å­è®¡ç®—å™¨
        momentum_factor = MomentumFactor()
        value_factor = ValueFactor()

        # å¯¹æ¯åªè‚¡ç¥¨è®¡ç®—å› å­å€¼
        for stock_code in stock_codes[:50]:  # é™åˆ¶æ•°é‡ä»¥æé«˜é€Ÿåº¦
            try:
                # åŠ è½½è‚¡ç¥¨æ•°æ®
                stock_files = list(self.stocks_dir.rglob(f"{stock_code}.csv"))
                if not stock_files:
                    continue

                all_data = []
                for file_path in stock_files:
                    df = pd.read_csv(file_path)
                    df['date'] = pd.to_datetime(df['date'])
                    all_data.append(df)

                combined_df = pd.concat(all_data, ignore_index=True)
                combined_df = combined_df.sort_values('date')
                combined_df = combined_df[
                    (combined_df['date'] >= start_date) &
                    (combined_df['date'] <= end_date)
                ]

                if combined_df.empty:
                    continue

                # è®¡ç®—å› å­å€¼
                market_data = {
                    'price_data': combined_df.to_dict('records'),
                    'current_price': float(combined_df.iloc[-1]['close']),
                    'volume': float(combined_df.iloc[-1]['volume'])
                }

                momentum_score = momentum_factor.calculate(stock_code, market_data)
                value_score = value_factor.calculate(stock_code, market_data)

                factor_results[stock_code] = {
                    'momentum_score': momentum_score,
                    'value_score': value_score,
                    'composite_score': momentum_score * 0.6 + value_score * 0.4,
                    'records': len(combined_df)
                }

            except Exception as e:
                logger.warning(f"è®¡ç®— {stock_code} å› å­æ—¶å‡ºé”™: {e}")
                continue

        # å› å­ç»Ÿè®¡åˆ†æ
        if factor_results:
            df_factors = pd.DataFrame(factor_results).T
            factor_stats = {
                'momentum_mean': df_factors['momentum_score'].mean(),
                'momentum_std': df_factors['momentum_score'].std(),
                'value_mean': df_factors['value_score'].mean(),
                'value_std': df_factors['value_score'].std(),
                'correlation': df_factors['momentum_score'].corr(df_factors['value_score'])
            }
        else:
            factor_stats = {}

        analysis_result = {
            'factor_scores': factor_results,
            'statistics': factor_stats,
            'analysis_period': {'start': start_date, 'end': end_date},
            'total_stocks': len(stock_codes),
            'analyzed_stocks': len(factor_results)
        }

        logger.info(f"å› å­åˆ†æå®Œæˆ: {len(factor_results)} åªè‚¡ç¥¨")
        return analysis_result

    def run_comprehensive_backtest(self, stock_codes: List[str]) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆå›æµ‹ç ”ç©¶"""
        logger.info("å¼€å§‹ç»¼åˆå›æµ‹ç ”ç©¶...")

        results = {}

        # åŸºç¡€å›æµ‹ï¼šåŠ¨é‡+ä»·å€¼å› å­ç»„åˆ
        base_config = {
            'momentum_weight': 0.6,
            'value_weight': 0.4,
            'rebalance_frequency': 'monthly'
        }

        try:
            base_result = self.backtest_engine.run_backtest(
                start_date=self.research_config['test_period'][0],
                end_date=self.research_config['test_period'][1],
                stock_universe=stock_codes[:self.research_config['universe_size']],
                rebalance_frequency=base_config['rebalance_frequency']
            )

            results['baseline_strategy'] = {
                'config': base_config,
                'performance': base_result['performance_metrics'],
                'trades': base_result['trades']
            }

        except Exception as e:
            logger.error(f"åŸºç¡€å›æµ‹å¤±è´¥: {e}")

        # ä¸åŒå› å­ç»„åˆå¯¹æ¯”
        combination_results = {}
        for i, factor_config in enumerate(self.research_config['factor_combinations']):
            config_name = f"combination_{i+1}"
            try:
                # è¿™é‡Œéœ€è¦ä¿®æ”¹å›æµ‹å¼•æ“ä»¥æ”¯æŒä¸åŒçš„å› å­æƒé‡
                # æš‚æ—¶ä½¿ç”¨åŸºç¡€é…ç½®ä½œä¸ºç¤ºä¾‹
                result = self.backtest_engine.run_backtest(
                    start_date=self.research_config['test_period'][0],
                    end_date=self.research_config['test_period'][1],
                    stock_universe=stock_codes[:self.research_config['universe_size']],
                    rebalance_frequency=self.research_config['rebalance_frequency']
                )

                combination_results[config_name] = {
                    'config': factor_config,
                    'performance': result['performance_metrics']
                }

            except Exception as e:
                logger.warning(f"å› å­ç»„åˆ {config_name} å›æµ‹å¤±è´¥: {e}")

        results['factor_combinations'] = combination_results

        # ç”Ÿæˆå¯¹æ¯”åˆ†æ
        if combination_results:
            comparison = self._compare_strategies(combination_results)
            results['strategy_comparison'] = comparison

        logger.info("ç»¼åˆå›æµ‹ç ”ç©¶å®Œæˆ")
        return results

    def _compare_strategies(self, strategies: Dict[str, Any]) -> Dict[str, Any]:
        """å¯¹æ¯”ä¸åŒç­–ç•¥è¡¨ç°"""
        comparison = {
            'performance_ranking': [],
            'risk_return_analysis': {},
            'best_performers': {}
        }

        performance_data = []

        for name, strategy in strategies.items():
            perf = strategy['performance']
            performance_data.append({
                'strategy': name,
                'config': strategy['config'],
                'total_return': perf.get('total_return', 0),
                'annualized_return': perf.get('annualized_return', 0),
                'sharpe_ratio': perf.get('sharpe_ratio', 0),
                'max_drawdown': perf.get('max_drawdown', 0),
                'win_rate': perf.get('win_rate', 0)
            })

        # æ’å
        df_perf = pd.DataFrame(performance_data)
        comparison['performance_ranking'] = df_perf.sort_values('sharpe_ratio', ascending=False).to_dict('records')

        # æœ€ä½³è¡¨ç°è€…
        if not df_perf.empty:
            comparison['best_performers'] = {
                'highest_return': df_perf.loc[df_perf['annualized_return'].idxmax()].to_dict(),
                'best_sharpe': df_perf.loc[df_perf['sharpe_ratio'].idxmax()].to_dict(),
                'lowest_drawdown': df_perf.loc[df_perf['max_drawdown'].idxmin()].to_dict()
            }

        return comparison

    def generate_research_report(self, data_quality: Dict, factor_analysis: Dict, backtest_results: Dict) -> str:
        """ç”Ÿæˆç ”ç©¶æŠ¥å‘Š"""
        report = f"""
# æ²ªæ·±300äº”å¹´æ•°æ®å›æµ‹ç ”ç©¶æŠ¥å‘Š

## 1. æ•°æ®æ¦‚å†µ
- æ•°æ®æ—¶é—´èŒƒå›´: {self.research_config['base_period'][0]} åˆ° {self.research_config['base_period'][1]}
- æœ‰æ•ˆè‚¡ç¥¨æ•°é‡: {data_quality['data_coverage']['valid_count']}/{data_quality['total_stocks']} ({data_quality['data_coverage']['validity_rate']:.1%})
- å¹³å‡äº¤æ˜“å¤©æ•°: {data_quality['quality_metrics'].get('avg_trading_days', 'N/A')}
- å¹³å‡æ•°æ®è´¨é‡åˆ†æ•°: {data_quality['quality_metrics'].get('avg_quality_score', 'N/A'):.3f}

## 2. å› å­åˆ†æç»“æœ
- åˆ†æè‚¡ç¥¨æ•°é‡: {factor_analysis['analyzed_stocks']}
- åŠ¨é‡å› å­å‡å€¼: {factor_analysis['statistics'].get('momentum_mean', 'N/A'):.3f}
- ä»·å€¼å› å­å‡å€¼: {factor_analysis['statistics'].get('value_mean', 'N/A'):.3f}
- å› å­ç›¸å…³æ€§: {factor_analysis['statistics'].get('correlation', 'N/A'):.3f}

## 3. å›æµ‹è¡¨ç°

### åŸºå‡†ç­–ç•¥ (åŠ¨é‡60% + ä»·å€¼40%)
"""

        if 'baseline_strategy' in backtest_results:
            base_perf = backtest_results['baseline_strategy']['performance']
            report += f"""
- æ€»æ”¶ç›Šç‡: {base_perf.get('total_return', 0):.2%}
- å¹´åŒ–æ”¶ç›Šç‡: {base_perf.get('annualized_return', 0):.2%}
- å¤æ™®æ¯”ç‡: {base_perf.get('sharpe_ratio', 0):.2f}
- æœ€å¤§å›æ’¤: {base_perf.get('max_drawdown', 0):.2%}
- èƒœç‡: {base_perf.get('win_rate', 0):.2%}
"""

        if 'strategy_comparison' in backtest_results:
            report += "\n### ç­–ç•¥å¯¹æ¯”æ’å\n"
            for i, strategy in enumerate(backtest_results['strategy_comparison']['performance_ranking'][:5], 1):
                report += f"{i}. {strategy['strategy']}: å¤æ™®æ¯”ç‡ {strategy['sharpe_ratio']:.2f}, å¹´åŒ–æ”¶ç›Š {strategy['annualized_return']:.2%}\n"

        report += f"""

## 4. ç ”ç©¶ç»“è®º
- æ•°æ®è´¨é‡: {'ä¼˜ç§€' if data_quality['data_coverage']['validity_rate'] > 0.8 else 'è‰¯å¥½' if data_quality['data_coverage']['validity_rate'] > 0.6 else 'éœ€è¦æ”¹è¿›'}
- å› å­æœ‰æ•ˆæ€§: {'æ˜¾è‘—' if abs(factor_analysis['statistics'].get('correlation', 0)) < 0.3 else 'ä¸­ç­‰ç›¸å…³'}
- ç­–ç•¥è¡¨ç°: {'ä¼˜å¼‚' if backtest_results.get('baseline_strategy', {}).get('performance', {}).get('sharpe_ratio', 0) > 1.5 else 'è‰¯å¥½' if backtest_results.get('baseline_strategy', {}).get('performance', {}).get('sharpe_ratio', 0) > 1.0 else 'éœ€è¦ä¼˜åŒ–'}

## 5. å»ºè®®
- ç»§ç»­æ‰©å……æ•°æ®è¦†ç›–èŒƒå›´ï¼Œæé«˜æ•°æ®è´¨é‡
- å°è¯•æ›´å¤šå› å­ç»„åˆå’Œæƒé‡é…ç½®
- è€ƒè™‘åŠ å…¥è¡Œä¸šè½®åŠ¨å’Œé£é™©æ§åˆ¶æœºåˆ¶
- å»¶é•¿å›æµ‹æ—¶é—´å‘¨æœŸéªŒè¯ç­–ç•¥ç¨³å¥æ€§

---
æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        # ä¿å­˜æŠ¥å‘Š
        report_file = self.reports_dir / f"research_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)

        logger.info(f"ç ”ç©¶æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        return report_file

    def run_full_research(self) -> str:
        """è¿è¡Œå®Œæ•´ç ”ç©¶æµç¨‹"""
        logger.info("ğŸš€ å¼€å§‹æ²ªæ·±300å®Œæ•´ç ”ç©¶æµç¨‹")

        # 1. åŠ è½½å¯ç”¨è‚¡ç¥¨
        available_stocks = self.load_available_stocks()

        # 2. æ•°æ®è´¨é‡éªŒè¯
        logger.info("ğŸ“Š æ­¥éª¤1: æ•°æ®è´¨é‡éªŒè¯")
        data_quality = self.validate_data_quality(available_stocks)

        if data_quality['data_coverage']['valid_count'] < 20:
            logger.error("å¯ç”¨æ•°æ®è´¨é‡è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆç ”ç©¶")
            return "æ•°æ®è´¨é‡ä¸è¶³"

        # 3. å› å­åˆ†æ
        logger.info("ğŸ“ˆ æ­¥éª¤2: å› å­åˆ†æ")
        valid_stocks = [s['stock'] for s in data_quality['valid_stocks']]
        factor_analysis = self.run_factor_analysis(
            valid_stocks,
            self.research_config['test_period'][0],
            self.research_config['test_period'][1]
        )

        # 4. å›æµ‹ç ”ç©¶
        logger.info("ğŸ”„ æ­¥éª¤3: å›æµ‹ç ”ç©¶")
        backtest_results = self.run_comprehensive_backtest(valid_stocks)

        # 5. ç”ŸæˆæŠ¥å‘Š
        logger.info("ğŸ“ æ­¥éª¤4: ç”Ÿæˆç ”ç©¶æŠ¥å‘Š")
        report_file = self.generate_research_report(data_quality, factor_analysis, backtest_results)

        logger.info("ğŸ‰ æ²ªæ·±300å®Œæ•´ç ”ç©¶æµç¨‹å®Œæˆ!")
        return str(report_file)


def main():
    parser = argparse.ArgumentParser(description="æ²ªæ·±300äº”å¹´æ•°æ®å›æµ‹ç ”ç©¶æ¡†æ¶")
    parser.add_argument("--data-dir", type=str, help="æ•°æ®ç›®å½•è·¯å¾„")
    parser.add_argument("--test", action="store_true", help="æµ‹è¯•æ¨¡å¼")

    args = parser.parse_args()

    print("ğŸ”¬ æ²ªæ·±300äº”å¹´æ•°æ®å›æµ‹ç ”ç©¶æ¡†æ¶")
    print("=" * 60)

    try:
        # åˆ›å»ºç ”ç©¶æ¡†æ¶
        framework = CSI300ResearchFramework(args.data_dir)

        if args.test:
            logger.info("ğŸ§ª æµ‹è¯•æ¨¡å¼")
            # è¿è¡Œç®€åŒ–çš„ç ”ç©¶æµç¨‹
            available_stocks = framework.load_available_stocks()[:20]
            data_quality = framework.validate_data_quality(available_stocks)
        else:
            # è¿è¡Œå®Œæ•´ç ”ç©¶æµç¨‹
            report_file = framework.run_full_research()
            print(f"\nğŸ“Š ç ”ç©¶æŠ¥å‘Š: {report_file}")

    except Exception as e:
        logger.error(f"ç ”ç©¶æµç¨‹å¤±è´¥: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()